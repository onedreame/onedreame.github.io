---
layout:     post
title:      subword系列算法
subtitle:   不同于传统分词的新奇方法
date:       2020-11-10
author:     OD
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - nlp
    - subword
    - tokenization
---

# subword系列算法

## 1. 前言

&emsp;*nlp*领域目前已经发展到一个非常高的层次了，这个层次不仅仅是模型和数据方面，还涌现出了很多非常巧妙的*trick*，这篇文章就记录一下关于*tokenization*方面的工作。

&emsp;所谓的*tokenization*其实就是将文本切分成*words*或者*subwords*，然后转成*ids*以便模型处理。最初的*nlp*分词非常简单，对于英语类以空格分割的语言来说，简单的以空格进行分割就行，不过这样简单的处理还存在一些问题，比如说*"Don’t you love 🤗 Transformers? We sure do."*,如果简单分割，那么就会得到*["Don't", "you", "love", "🤗", "Transformers?", "We", "sure", "do."]*，对于*“Transformers?”* 或者 *“do.”*这样的分词结果，显然是不合适，所以还需要考虑一下符号的影响，这可以通过现在很多开源工具实现，比如*[spaCy](https://spacy.io/)*或者[Moses](http://www.statmt.org/moses/?n=Development.GetStarted)等等。不过很明显会有个问题，那就是你切分的越细，得到的*vocabulary*越大，对于存储空间的要求也越大，虽然大部分情况下我们的词典都不会特别大，但是总是存在意外情况，尤其是[Bert](https://onedreame.github.io/2020/10/31/bert%E5%AE%B6%E6%97%8F/)这类对于语料越大效果越好的模型的出现，更是加剧了这种情况。如果切分成*words*会导致内存问题，那么一个想法就是做*character*级别的*tokenization*，对于英文来说只有26个，那么算上大小写也只有52个，这可以大大缓解过大的*embedding matrix*问题，但是，*character*级别需要模型去从序列顺序中学习*word*的表示，模型在这方面显然不如直接切分*word*来的直观，所以这类方法会有性能方面的损失。在这种情况下，混合了*word*和*character*的一些新方法应运而出，这类方法被称为*subword*。

## 2.*subword* *tokenization*

### 2.1 [*Byte Pair Encoding*](https://arxiv.org/abs/1508.07909)

&emsp;这种算法其实很简单，从其名字基本可以了解到一些东西，最显眼的就是*pair*这个词。它的做法是每次使用频次最高的一个字节对来替换这两个字节。

&emsp;算法流程：

1. 准备足够大的训练语料
2. 确定期望的*subword*词表大小
3. 将单词拆分为字符序列并在末尾添加后缀“ </ w>”（这里不是必须，可以添加任意可以代表结束的字符），统计单词频率。 本阶段的*subword*的粒度是字符。 例如，“ *low*”的频率为5，那么我们将其改写为“ l o w </ w>”：5
4. 统计每一个连续字节对的出现频率，选择最高频者合并成新的*subword*
5. 重复第4步直到达到第2步设定的*subword*词表大小或下一个最高频的字节对出现频率为1

![](https://miro.medium.com/max/970/1*_bpIUb6YZr6DOMLAeSU2WA.png)

#### 2.1.1 构建*subword*字典

&emsp;有输入的*word*和其频次，首先将*word*分成*character*，并将*subword*词典初始化为*character*：

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}
```

&emsp;*Iter 1*, 最高频连续字节对"*e*"和"*s*"出现了6+3=9次，合并成"*es*",将"*es*"加入*subword*词典。输出：

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}
```

&emsp;*Iter 2*, 最高频连续字节对"*es*"和"*t*"出现了6+3=9次, 合并成"*est*"，并将"*est*"加入*subword*词典。输出：

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}
```

&emsp;Iter 3, 以此类推，最高频连续字节对为"*est*"和"*</ w>*" 输出，同样加入*subword*词典：

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}
```

&emsp;……

&emsp;Iter n, 继续迭代直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1。

#### 2.1.2 编码

&emsp;根据2.1.1得到了*subword*的字典，对单词就可以进行编码了，将字典按照字符长度递减排列，然后依次遍历词典中的*subword*，如果单词包含了该*subword*，那么就用该*subword*加入单词的分解集合中，对剩余的字符进行迭代。这里将词典首先排序，便可以保证最先加入集合的*subword*是最后生成的。若最后还有一些字符序列无法在字典中找到，则替换为特殊的*subword*，比如*< unk >*.

&emsp;同样举个例子, 已有排好序的词典:

```text
[“errrr</w>”, “tain</w>”, “moun”, “est</w>”, “high”, “the</w>”, “a</w>”]
```

&emsp;对单词"*mountain</ w>*",遍历词典，首先看到“tain</ w>”出现在了单词中，加入该*subword*，对剩余的"*moun*"继续便利，下一个*subword* “*moun*”匹配，加入，那么便得到了单词的分解结果为：*["moun", "tain</ w>"]*。

#### 2.1.3 解码

&emsp;将*tokens*拼接在一起即可，注意"< /w>"代表句子结束。

```python
# 编码序列
[“the</w>”, “high”, “est</w>”, “moun”, “tain</w>”]

# 解码序列
“the</w> highest</w> mountain</w>”
```

### 2.2 [*WordPiece*](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)

&emsp;*wordpiece*思路基本与*Byte Pair Encoding*，唯一的不同就是在进行合并tokens对的时候不是使用*max frequency*，而是使用概率来确定合并哪两个tokens对，计算公式为：


$$
score(A,B)=Frequency(A,b)/(Frequency(A)*Frequency(B))
$$
&emsp;取score最大的tokens对进行合并，这么做也很直观，类似于词袋模型中的tfidf。

&emsp;*transformers* 库中的*BERT tokenizer*即是使用的这种方法：

```shell
>>> from transformers import BertTokenizer
>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

>>> sequence = "A Titan RTX has 24GB of VRAM"
>>> tokenized_sequence = tokenizer.tokenize(sequence)
>> print(tokenized_sequence)
['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']
```

&emsp;"##"代表当前序列是前面字节的后缀，比如'R', '##T', '##X'三个序列，应该拼合为"RTX".这里没有使用结束符号，而是使用拼接符号来确保单词何时结束。

### 2.3 [Unigram Language Model](https://arxiv.org/pdf/1804.10959.pdf)

&emsp;这是一种基于语言模型的*tokenization*方式，不同于*BPE*或者*WordPiece*,该方法不是从基础字符开始然后通过一定的规则进行合并，这种方法通过一个很大的词典开始，然后渐进的缩减词典规模。生成词典的具体流程如下：

![](https://miro.medium.com/max/712/1*9CPFFg-ilQrcPfWT8J6_ug.png)

<center>P(x):P(subwords), V:词典, L:Likelihood to be maximized</center>

其中，L的计算公式为：

![](https://miro.medium.com/max/1012/1*3Uwgh_Z8uSX13jHZBoNjrA.png)

其中，$|D|$为词典的大小，$S(x)$为词*x*的所有可能的*tokenization*组合集合。

&emsp;得到词典以后，就是进行*tokenization*了，不过要注意的是*tokenization*的方式可能不止一种，比如有以下的词典：

```python
['b', 'g', 'h', 'n', 'p', 's', 'u', 'ug', 'un', 'hug']
```

那么对于单词“*hugs*” ，可以有如下的几种组合： `['hug', 's']`, `['h', 'ug', 's']` 或者 `['h', 'u', 'g', 's']`，由于生成字典的时候每个*subword*都是有对应的概率的，所以可以取tokens概率乘积最大的组合或者以概率随机取一个组合。

### 2.4 [SentencePiece](https://arxiv.org/pdf/1808.06226.pdf)

&emsp;仔细看一下上面的几种方法可以发现，他们都是需要实现构建一个词典，然后迭代，而且上面几种方法对于没有分隔符的语言比如中文来说并不适用，因为对于中文这类语言来说，单个字已经是最小单元了，无法细分为更小的单元了。所以上面几种方法都是与语言相关的，那么，有没有*language-independent* 的方法呢？当当当，这时候就到了*SentencePiece*粉墨登场的时候了。

&emsp;*SentencePiece*主要由4个模块组成：*Normalizer,Trainer,Encoder,*  以及*Decoder*。*Normalizer*是一个将语义等价的Unicode字符归一化为规范形式的模块。  *Trainer*从归一化语料中训练*subword*分割模型，一般使用BPE或者unigram。 *Encoder*内部执行*Normalizer*对输入文本进行归一化，并将输入文本用*Trainer*训练的*subword*模型标记成*subword*序列。 *Decoder*将*subword*序列转换为归一化文本。这四个模块有如下的关系：*Decoder( Encoder (Normalizer(corpus))) = Normalizer (corpus)*，作者称这为**lossless tokenization**。通过这些模块，*SentencePiece*实现了一种端到端的*tokenization*。

&emsp;*transformers*库的*XlNet*，*ALBERT*等使用了这种方法：

```shell
>>> from transformers import XLNetTokenizer
>>> tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
>>> tokenizer.tokenize("Don't you love 🤗 Transformers? We sure do.")
['▁Don', "'", 't', '▁you', '▁love', '▁', '🤗', '▁', 'Transform', 'ers', '?', '▁We', '▁sure', '▁do', '.']
```

其中的 ‘▁’ 代表空格，在*decode*的时候将该符号替换回空格即可。

## 3.总结

- 传统词表示方法无法很好的处理未知或罕见的词汇（OOV问题）

- 传统词*tokenization*方法不利于模型学习词缀之间的关系

- - E.g. 模型学到的“*old*”, “*older*”, *and* “*oldest*”之间的关系无法泛化到“*smart*”, “*smarter*”, and “*smartest*”。

- *Character* *embedding*作为*OOV*的解决方法粒度太细

- *Subword*粒度在词与字符之间，能够较好的平衡*OOV*问题